package dev.finprotect;

import java.util.HashMap;
import java.util.HashSet;
import java.util.Arrays;
import java.util.Map;
import java.util.Set;
import java.util.regex.Pattern;
import java.util.List;
import java.util.Collection;

import scala.Tuple2;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.common.serialization.StringDeserializer;

import org.apache.spark.api.java.*;
import org.apache.spark.SparkConf;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.streaming.api.java.*;
import org.apache.spark.streaming.kafka010.ConsumerStrategies;
import org.apache.spark.streaming.kafka010.KafkaUtils;
import org.apache.spark.streaming.kafka010.LocationStrategies;
import org.apache.spark.streaming.Durations;


import org.apache.spark.sql.*;
import org.apache.spark.sql.functions;
import org.apache.spark.sql.streaming.*;
import org.apache.spark.sql.kafka010.*;
import org.apache.spark.sql.types.*;


public final class fpMain
{
	private static final Pattern delim = Pattern.compile(",");
	public static void main(String args[]) throws Exception
	{
		SparkSession spark = SparkSession
		      .builder()
		      .master("local[*]")
		      .appName("fpMain")
		      .config("spark.jars","/home/fprotect/finprotect/fprotect/target/fprotect-0.1.jar")
		      .getOrCreate();
		
		StructType trSchema = new StructType().add("id","integer").add("type","string").add("amount","float").add("nameOrig","string").add("oldBalanceOrig","float").add("newBalanceOrig","float").add("nameDest","string").add("oldBalanceDest","float").add("newBalanceDest","float").add("isFraud","integer").add("isFlaggedFraud","integer");
		Dataset<Row> df = spark
				.readStream()
				.format("kafka")
				.option("kafka.bootstrap.servers","localhost:9092")
				.option("subscribe","fp_trdata")
				.option("startingOffsets","earliest")	//testing only
				.option("failOnDataLoss","false")
				.load();
		df.printSchema();
		//df = df.selectExpr("CAST(value AS STRING)");
		df = df.select(functions.from_json(df.col("value").cast("string"),trSchema));
		df = df.select("jsontostructs(CAST(value AS STRING)).*");
		/*df = df.selectExpr("CAST(split(value,',')[0] as integer) as step",		//use json instead
		 		"CAST(split(value,',')[1] as string) as type",
		 		"CAST(split(value,',')[2] as float) as amount",
		 		"CAST(split(value,',')[3] as string) as nameOrig",
		 		"CAST(split(value,',')[4] as float) as oldBalanceOrig",
		 		"CAST(split(value,',')[5] as float) as newBalanceOrig",
		 		"CAST(split(value,',')[6] as string) as nameDest",
		 		"CAST(split(value,',')[7] as float) as oldBalanceDest",
		 		"CAST(split(value,',')[8] as float) as newBalanceDest",
		 		"CAST(split(value,',')[9] as integer) as isFraud",
		 		"CAST(split(value,',')[10] as integer) as isFlaggedFraud");*/
		 		
		//Dataset<Row> df_det = df.select("*").where("newBalanceOrig < 100.0");
    		
    		
  		/*StreamingQuery dsw = df.writeStream()
    				.outputMode("append")
    				.format("console")
    				.start();
    		dsw.awaitTermination();*/
    	
    		/*StreamingQuery dsw = df.writeStream()
    				.format("csv")
    				.option("path","/home/fprotect/finprotect/fprotect/out")
    				.option("checkpointLocation","/home/fprotect/finprotect/fprotect/checkpoints")
    				.outputMode("append")
    				.start();
    		dsw.awaitTermination();*/
    		
    		df.printSchema();
    		
    		StreamingQuery dsw = df.select("CAST functions.to_json(struct(*)) AS value")
    				.writeStream()
				.format("kafka")
				.option("kafka.bootstrap.servers","localhost:9092")
				.option("topic","fp_trdata_det")
				.option("checkpointLocation","/home/fprotect/finprotect/fprotect/checkpoints")
				.start();
		dsw.awaitTermination();
		
	}
	
}


